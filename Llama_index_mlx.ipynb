{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b13f46-031a-447e-864b-d9f6e7c90c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Mapping, Any\n",
    "from mlx_lm import load, generate\n",
    "from llama_index.core import SimpleDirectoryReader, SummaryIndex, Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback \n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class OurLLM(CustomLLM, BaseModel):\n",
    "    model: Optional[Any] = None\n",
    "    tokenizer: Optional[Any] = None\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)  # Initialize BaseModel part with data\n",
    "        # Directly load the model and tokenizer\n",
    "        self.model, self.tokenizer = load(\"mlx-community/Mistral-7B-v0.1-hf-4bit-mlx\")\n",
    "    context_window: int = 4096\n",
    "    max_tokens : int = 500\n",
    "    model_name: str = \"custom\"\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window = self.context_window,\n",
    "            model_name=self.model_name,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "\n",
    "    def process_generated_text(self, text: str) -> str:\n",
    "        token_pos = text.find(\"\\n\\n\")\n",
    "        if token_pos != -1:\n",
    "            # Truncate text at the first occurrence of two new lines\n",
    "            return text[:token_pos]\n",
    "        return text\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "       # Remove 'formatted' argument if present\n",
    "        kwargs.pop('formatted', None)\n",
    "    \n",
    "        generated_text = generate(self.model, self.tokenizer, prompt=prompt, verbose=False, **kwargs)\n",
    "        processed_text = self.process_generated_text(generated_text)\n",
    "        return CompletionResponse(text=processed_text)\n",
    "\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        generated_text = generate(self.model, self.tokenizer, prompt=prompt, verbose=False, **kwargs)\n",
    "        processed_text = self.process_generated_text(generated_text)\n",
    "        for char in processed_text:  \n",
    "            yield CompletionResponse(text=char, delta=char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaaccc1-60a4-4956-a678-cb7a49be580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e84dedbd8a49e89adfa440b477ca75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our LLM\n",
    "Settings.llm = OurLLM()\n",
    "\n",
    "# Define embed model\n",
    "Settings.embed_model = \"local:BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c53137d-1202-4747-87c5-afe266f55c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eccc5a5fb324639a2565ae90ffbce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb75542fb1340108f139be1dbbc826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f139f99c-63d7-45b4-a4b2-376440682bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We the People of the United States, in Order to form a  more perfect Union, establish Justice, insure domestic  Tranquility, provide for the common defence, promote the general  Welfare, and secure the Bless Liberty to ourselves  and our Posterity,  do ordain  and establish this Constitution for the United States of America\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the first sentence of the constitution\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da2d1a-9823-44fa-89f8-8c6a4c811e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
